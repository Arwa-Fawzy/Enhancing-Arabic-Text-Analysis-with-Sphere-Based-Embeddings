{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KLi-WYjIjqdc"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar, normalize_teh_marbuta_ar\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MrZAh4u7jqde"
      },
      "outputs": [],
      "source": [
        "input_file_path = r'/content/part_0.txt'\n",
        "output_file_path = r'/content/part_0_output.txt'\n",
        "\n",
        "# Cleaning the data from symbols of XML\n",
        "def remove_xml_tags(text):\n",
        "    clean_text = re.sub(r'<[^>]+>', '', text)\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oF1rUp-Ijqdf"
      },
      "outputs": [],
      "source": [
        "# Reading the file\n",
        "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "    content = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "a6X63m8qjqdf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Removing XML Symbols\n",
        "clean_content = remove_xml_tags(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "d7WKBhlfjqdf"
      },
      "outputs": [],
      "source": [
        "# Normalizing the Text\n",
        "normalized_content = normalize_alef_maksura_ar(normalize_teh_marbuta_ar(clean_content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "v0mvywpbjqdf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tokenizing the text\n",
        "tokens = simple_word_tokenize(normalized_content)\n",
        "\n",
        "#Converting the list of words into text again\n",
        "tokenized_content = ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qxdzsafbjqdg",
        "outputId": "3457e65f-4cc7-48b3-f371-789004cae9e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The normalized and tokenized content has been saved to /content/part_0_output.txt\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Saving the output\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    output_file.write(tokenized_content)\n",
        "\n",
        "print(f'The normalized and tokenized content has been saved to {output_file_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnrDgaZujqdg",
        "outputId": "07c926d1-7dc3-4c41-b14e-e73e7962933b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file /content/part_0_output.txt has been cleaned from non-Arabic characters and symbols.\n"
          ]
        }
      ],
      "source": [
        "def remove_non_arabic(text):\n",
        "    arabic_text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
        "    return arabic_text\n",
        "\n",
        "with open(output_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    content = file.read()\n",
        "\n",
        "clean_arabic_content = remove_non_arabic(content)\n",
        "\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    output_file.write(clean_arabic_content)\n",
        "\n",
        "print(f'The file {output_file_path} has been cleaned from non-Arabic characters and symbols.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tekn5_ID3Djs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}